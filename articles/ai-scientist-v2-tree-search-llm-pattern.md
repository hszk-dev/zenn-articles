---
title: "AI Scientist-v2ã‹ã‚‰å­¦ã¶ã€Œæ¢ç´¢çš„ç”Ÿæˆã€ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ - Tree Searchã¨LLMã§è¨˜äº‹å“è³ªã‚’å®‰å®šå‘ä¸Šã•ã›ã‚‹æŠ€è¡“"
emoji: "ğŸŒ³"
type: "tech"
topics: ["LLM", "AI", "TreeSearch", "Python", "æ©Ÿæ¢°å­¦ç¿’"]
published: true
---

## ã¯ã˜ã‚ã«

LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®é–‹ç™ºã«ãŠã„ã¦ã€å˜ç™ºç”Ÿæˆã«ã‚ˆã‚‹å‡ºåŠ›å“è³ªã®ã°ã‚‰ã¤ãã¯æ·±åˆ»ãªèª²é¡Œã§ã™ã€‚ã€Œä»Šå›ã¯è‰¯ã„çµæœãŒå‡ºãŸãŒã€æ¬¡å›ã¯æœŸå¾…å¤–ã‚Œã€ã¨ã„ã£ãŸä¸å®‰å®šæ€§ã¯ã€ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã®ä¿¡é ¼æ€§ã‚’å¤§ããæãªã„ã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€AI Scientist-v2ã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹ã€Œæ¢ç´¢çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ã‚’**è¨˜äº‹ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«å¿œç”¨**ã™ã‚‹æ‰‹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚AI Scientist-v2ã¯æœ¬æ¥ã€ç§‘å­¦å®Ÿé¨“ã®è‡ªå‹•åŒ–ï¼ˆã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ»å®Ÿè¡Œãƒ»æ”¹å–„ï¼‰ã«Tree Searchã‚’æ´»ç”¨ã—ã¦ã„ã¾ã™ãŒã€ã“ã®æ¢ç´¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æœ¬è³ªã§ã‚ã‚‹ã€Œå€™è£œå±•é–‹â†’è©•ä¾¡â†’é¸æŠâ†’å‰ªå®šã€ã®ã‚µã‚¤ã‚¯ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã«ã‚‚æœ‰åŠ¹ã«é©ç”¨ã§ãã¾ã™ã€‚

èª­è€…ã®çš†æ§˜ã¯ã€å¾“æ¥ã®ä¸€ç™ºå‹è² çš„ãªç”Ÿæˆã‹ã‚‰è„±å´ã—ã€å®‰å®šã—ãŸé«˜å“è³ªãªå‡ºåŠ›ã‚’å¾—ã‚‹ãŸã‚ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ã¹ã¾ã™ã€‚å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰ä¾‹ã¨å…±ã«ã€è¨˜äº‹ç”Ÿæˆã®å“è³ªã‚’å®‰å®šå‘ä¸Šã•ã›ã‚‹å…·ä½“çš„ãªæŠ€è¡“ã‚’èº«ã«ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã§ã—ã‚‡ã†ã€‚

## ãªãœLLMã«ã€Œæ¢ç´¢ã€ãŒå¿…è¦ãªã®ã‹ - å˜ç™ºç”Ÿæˆã®é™ç•Œ

![ãƒ„ãƒªãƒ¼ã‚µãƒ¼ãƒ](/images/ãƒ„ãƒªãƒ¼ã‚µãƒ¼ãƒ.jpg)

LLMã®å‡ºåŠ›ã¯ç¢ºç‡çš„ãªæ€§è³ªã‚’æŒã¤ãŸã‚ã€åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚æ¯å›ç•°ãªã‚‹çµæœãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚ç‰¹ã«æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtemperatureï¼‰ã‚’é«˜ãè¨­å®šã™ã‚‹ã¨ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªå‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ä¸€æ–¹ã§ã€ä¸€è²«æ€§ãŒå¤±ã‚ã‚ŒãŒã¡ã§ã™ã€‚é€†ã«æ¸©åº¦ã‚’ä½ãã™ã‚‹ã¨å®‰å®šæ€§ã¯å‘ä¸Šã—ã¾ã™ãŒã€å‰µé€ æ€§ã«æ¬ ã‘ã‚‹å¹³å‡¡ãªå‡ºåŠ›ã«ãªã‚Šã‚„ã™ã„ã¨ã„ã†æ ¹æœ¬çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒå­˜åœ¨ã—ã¾ã™ã€‚

ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã¯ã€ã“ã®å“è³ªã®ã°ã‚‰ã¤ããŒå¤§ããªå•é¡Œã¨ãªã‚Šã¾ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«æä¾›ã™ã‚‹è¨˜äº‹ã‚„ãƒ¬ãƒãƒ¼ãƒˆã®å“è³ªãŒå®Ÿè¡Œã”ã¨ã«å¤§ããå¤‰å‹•ã™ã‚‹ã®ã¯ã€ãƒ“ã‚¸ãƒã‚¹ä¸Šå—ã‘å…¥ã‚Œã‚‰ã‚Œã¾ã›ã‚“ã€‚å¾“æ¥ã®å˜ç™ºç”Ÿæˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€ä¸€åº¦ã®ç”Ÿæˆã§æœ€é«˜å“è³ªã®å‡ºåŠ›ã‚’å¾—ã‚‹ã“ã¨ã¯å›°é›£ã§ã‚ã‚Šã€è¤‡æ•°ã®å€™è£œã‹ã‚‰æœ€é©è§£ã‚’é¸æŠã™ã‚‹ã€Œæ¢ç´¢çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ãŒå¿…è¦ã¨ãªã‚‹ã®ã§ã™ã€‚

```python
import openai
import numpy as np
from typing import List, Dict

def compare_temperature_outputs(prompt: str, temperatures: List[float], n_samples: int = 5) -> Dict[float, List[str]]:
    """
    ç•°ãªã‚‹æ¸©åº¦è¨­å®šã§ã®LLMå‡ºåŠ›å“è³ªã®ã°ã‚‰ã¤ãã‚’æ¯”è¼ƒ
    """
    client = openai.OpenAI()
    results = {}
    
    for temp in temperatures:
        outputs = []
        for _ in range(n_samples):
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=temp,
                max_tokens=100
            )
            outputs.append(response.choices[0].message.content)
        results[temp] = outputs
    
    return results

# å®Ÿéš›ã®æ¯”è¼ƒå®Ÿè¡Œä¾‹
prompt = "AIã®æœªæ¥ã«ã¤ã„ã¦100æ–‡å­—ç¨‹åº¦ã§èª¬æ˜ã—ã¦ãã ã•ã„"
temperatures = [0.1, 0.7, 1.2]
results = compare_temperature_outputs(prompt, temperatures)

# çµæœã®åˆ†æ
for temp, outputs in results.items():
    print(f"\næ¸©åº¦: {temp}")
    print(f"å‡ºåŠ›ã®å¤šæ§˜æ€§: {len(set(outputs))} / {len(outputs)}")
    for i, output in enumerate(outputs[:2]):
        print(f"  ä¾‹{i+1}: {output[:50]}...")
```

## AI Scientist-v2ã®Tree Searchã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è§£èª¬

![æ¢ç´¢ã‚µã‚¤ã‚¯ãƒ«](/images/æ¢ç´¢ã‚µã‚¤ã‚¯ãƒ«.jpg)

AI Scientist-v2ã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹Best-First Tree Searchï¼ˆæœ€è‰¯å„ªå…ˆæœ¨æ¢ç´¢ï¼‰ã¯ã€**ç§‘å­¦å®Ÿé¨“ãƒ—ãƒ­ã‚»ã‚¹ã®è‡ªå‹•æ¢ç´¢**ã‚’å®Ÿç¾ã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚é‡è¦ãªç‚¹ã¨ã—ã¦ã€ã“ã®Tree Searchã¯è«–æ–‡ã®ã€ŒåŸ·ç­†ã€ã§ã¯ãªãã€**å®Ÿé¨“ã®è¨­è¨ˆãƒ»å®Ÿè¡Œãƒ»æ”¹å–„**ã«é©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

æœ¬å®¶ã®å®Ÿè£…ã§ã¯ã€å„ãƒãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®è¦ç´ ã‚’ä¿æŒã—ã¾ã™ï¼š

- `code`: å®Ÿé¨“ã‚’å®Ÿè¡Œã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰
- `plan`: å®Ÿé¨“è¨ˆç”»ã®èª¬æ˜
- `metric`: å®Ÿé¨“çµæœï¼ˆvalidation lossãªã©ï¼‰
- `is_buggy`: ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã®æœ‰ç„¡

æ¢ç´¢ãƒ—ãƒ­ã‚»ã‚¹ã¯4ã¤ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§æ§‹æˆã•ã‚Œã¾ã™ï¼š1ï¼‰initial_implementationï¼ˆåŸºæœ¬å®Ÿè£…ï¼‰ã€2ï¼‰baseline_tuningï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼‰ã€3ï¼‰creative_researchï¼ˆæ–°è¦æ‰‹æ³•æ¢ç´¢ï¼‰ã€4ï¼‰ablation_studiesï¼ˆã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ï¼‰ã€‚å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§è¤‡æ•°ã®å®Ÿé¨“ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆãƒ»è©•ä¾¡ã—ã€æœ€ã‚‚æœ‰æœ›ãªçµæœã‚’ç¤ºã—ãŸãƒãƒ¼ãƒ‰ã‚’å„ªå…ˆçš„ã«å±•é–‹ã—ã¾ã™ã€‚

ã“ã®ã€Œå€™è£œå±•é–‹â†’è©•ä¾¡â†’é¸æŠâ†’å‰ªå®šã€ã¨ã„ã†ã‚µã‚¤ã‚¯ãƒ«ã®æœ¬è³ªã¯ã€**æ¢ç´¢å¯¾è±¡ã‚’å¤‰ãˆã¦ã‚‚æœ‰åŠ¹**ã§ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€å®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã®ä»£ã‚ã‚Šã«è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã€å®Ÿé¨“metricã®ä»£ã‚ã‚Šã«å“è³ªã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€åŒæ§˜ã®æ¢ç´¢çš„æ”¹å–„ã‚’è¨˜äº‹ç”Ÿæˆã«å¿œç”¨ã—ã¾ã™ã€‚

## æœ¬å®¶AI Scientist-v2ã¨ã®å®Ÿè£…ä¸Šã®é•ã„

æœ¬è¨˜äº‹ã§ç´¹ä»‹ã™ã‚‹å®Ÿè£…ã¯ã€AI Scientist-v2ã®æ¢ç´¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’**è¨˜äº‹ç”Ÿæˆã‚¿ã‚¹ã‚¯å‘ã‘ã«ã‚¢ãƒ€ãƒ—ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**ã—ãŸã‚‚ã®ã§ã™ã€‚ä¸¡è€…ã®é•ã„ã‚’æ˜ç¢ºã«ã—ã¦ãŠãã¾ã—ã‚‡ã†ã€‚

| é …ç›® | AI Scientist-v2ï¼ˆå®Ÿé¨“æ¢ç´¢ï¼‰ | æœ¬å®Ÿè£…ï¼ˆè¨˜äº‹ç”Ÿæˆï¼‰ |
|------|---------------------------|-------------------|
| **æ¢ç´¢å¯¾è±¡** | å®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã¨çµæœ | è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ |
| **ãƒãƒ¼ãƒ‰ã®ä¸­èº«** | `code`, `plan`, `metric`, `is_buggy` | `content`, `sections`, `score` |
| **è©•ä¾¡åŸºæº–** | å®Ÿé¨“metricï¼ˆvalidation lossãªã©ï¼‰ | æ–‡ç« å“è³ªã‚¹ã‚³ã‚¢ï¼ˆæ§‹é€ ãƒ»æ­£ç¢ºæ€§ãƒ»èª­ã¿ã‚„ã™ã•ï¼‰ |
| **ä¸¦è¡Œå‡¦ç†** | `multiprocessing`ï¼ˆã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã®ãŸã‚ï¼‰ | `asyncio`ï¼ˆAPIå‘¼ã³å‡ºã—ã¯IOãƒã‚¦ãƒ³ãƒ‰ï¼‰ |
| **ãƒãƒ¼ãƒ‰å±•é–‹** | debugï¼ˆãƒã‚°ä¿®æ­£ï¼‰/ improveï¼ˆæ”¹å–„ï¼‰ | æ¸©åº¦å¤‰åŒ–ãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰åŒ–ã«ã‚ˆã‚‹ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ |

**ãªãœã“ã®ã‚¢ãƒ€ãƒ—ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒæœ‰åŠ¹ã‹ï¼Ÿ**

æ¢ç´¢çš„ç”Ÿæˆã®æœ¬è³ªã¯ã€Œè¤‡æ•°ã®å€™è£œã‚’ç”Ÿæˆã—ã€è©•ä¾¡ã«åŸºã¥ã„ã¦æœ€é©è§£ã‚’é¸æŠã™ã‚‹ã€ã“ã¨ã«ã‚ã‚Šã¾ã™ã€‚AI Scientist-v2ã§ã¯å®Ÿé¨“çµæœï¼ˆæ•°å€¤metricï¼‰ã§è©•ä¾¡ã—ã¾ã™ãŒã€è¨˜äº‹ç”Ÿæˆã§ã¯LLMã«ã‚ˆã‚‹å“è³ªè©•ä¾¡ã§åŒæ§˜ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã‚’æ§‹ç¯‰ã§ãã¾ã™ã€‚

ã¾ãŸã€æœ¬å®¶ã§ã¯`multiprocessing`ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€ã“ã‚Œã¯å®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡ŒãŒCPUãƒã‚¦ãƒ³ãƒ‰ã ã‹ã‚‰ã§ã™ã€‚ä¸€æ–¹ã€è¨˜äº‹ç”Ÿæˆã§ã¯LLM APIã®å‘¼ã³å‡ºã—ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã‚‹ãŸã‚ã€`asyncio`ã«ã‚ˆã‚‹éåŒæœŸå‡¦ç†ãŒã‚ˆã‚Šé©åˆ‡ã§ã™ã€‚

### ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç‰¹æœ‰ã®èª²é¡Œ

**1. ã€Œãƒ‡ãƒãƒƒã‚°ã€ã«ç›¸å½“ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ä¸åœ¨**

AI Scientist-v2ã§ã¯ã€ç”Ÿæˆã—ãŸã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã€ã‚¨ãƒ©ãƒ¼ï¼ˆTracebackï¼‰ãŒå‡ºã‚Œã°ä¿®æ­£ã™ã‚‹ã¨ã„ã†**å®¢è¦³çš„ãªå¤±æ•—ä¿¡å·**ã«åŸºã¥ããƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ãŒå­˜åœ¨ã—ã¾ã™ã€‚ä¸€æ–¹ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã§ã¯ã€Œå‹•ã/å‹•ã‹ãªã„ã€ã¨ã„ã†æ˜ç¢ºãªå¢ƒç•Œç·šãŒã‚ã‚Šã¾ã›ã‚“ã€‚

æœ¬å®Ÿè£…ã§ã¯ã€ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®æ§‹æ–‡æ¤œè¨¼ï¼ˆ`CodeValidator`ï¼‰ã‚„å®Ÿè¡Œæ¤œè¨¼ï¼ˆ`DockerSandbox`ï¼‰ã‚’æ¢ç´¢ãƒ«ãƒ¼ãƒ—ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€éƒ¨åˆ†çš„ã«ã“ã®èª²é¡Œã«å¯¾å‡¦ã§ãã¾ã™ã€‚

**2. è©•ä¾¡ã®å®¢è¦³æ€§ï¼ˆLLMãŒLLMã‚’è©•ä¾¡ã™ã‚‹å•é¡Œï¼‰**

AI Scientist-v2ã®è©•ä¾¡æŒ‡æ¨™ã¯Validation Lossãªã©ã®**æ•°å€¤çš„Ground Truth**ã§ã™ã€‚æœ¬å®Ÿè£…ã§ã¯LLMã«ã‚ˆã‚‹å“è³ªè©•ä¾¡ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€ã€ŒLLMãŒå¥½ã¿ã‚„ã™ã„æ–‡ç« ã€ãŒé«˜è©•ä¾¡ã•ã‚Œã‚‹å¯èƒ½æ€§ï¼ˆReward Hackingï¼‰ãŒã‚ã‚Šã¾ã™ã€‚

å¯¾ç­–ã¨ã—ã¦ã€äººé–“è©•ä¾¡ã¨ã®ç›¸é–¢æ¤œè¨¼ã‚„ã€å¤–éƒ¨æŒ‡æ¨™ï¼ˆæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ç‡ã€å®Ÿè¡ŒæˆåŠŸç‡ï¼‰ã®ä½µç”¨ãŒæœ‰åŠ¹ã§ã™ã€‚

## å®Ÿè£…ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ - SearchTreeã€TreeNodeã€ArticleState

æ¢ç´¢çš„ç”Ÿæˆã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã¯ã€çŠ¶æ…‹ç®¡ç†ã€æ¢ç´¢åˆ¶å¾¡ã€å€™è£œè©•ä¾¡ã®3ã¤ã®è¦ç´ ã‚’é©åˆ‡ã«åˆ†é›¢ã—ãŸè¨­è¨ˆãŒé‡è¦ã§ã™ã€‚SearchTreeã‚¯ãƒ©ã‚¹ã¯æ¢ç´¢å…¨ä½“ã®åˆ¶å¾¡ã‚’æ‹…ã„ã€å„ãƒãƒ¼ãƒ‰ã®å±•é–‹é †åºã‚„å‰ªå®šï¼ˆpruningï¼‰æˆ¦ç•¥ã‚’ç®¡ç†ã—ã¾ã™ã€‚TreeNodeã‚¯ãƒ©ã‚¹ã¯å€‹ã€…ã®å€™è£œçŠ¶æ…‹ã‚’è¡¨ç¾ã—ã€è¦ªå­é–¢ä¿‚ã¨ã‚¹ã‚³ã‚¢æƒ…å ±ã‚’ä¿æŒã™ã‚‹ã“ã¨ã§ã€æ¢ç´¢æœ¨ã®æ§‹é€ ã‚’ç¶­æŒã—ã¾ã™ã€‚ArticleStateã‚¯ãƒ©ã‚¹ã¯è¨˜äº‹ã®éƒ¨åˆ†çš„ãªå®ŒæˆçŠ¶æ…‹ã‚’è¡¨ç¾ã—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ§‹æˆã‚„å†…å®¹ã®å¤‰æ›´ã‚’è¿½è·¡å¯èƒ½ã«ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ã‚¯ãƒ©ã‚¹é–“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æ˜ç¢ºã«å®šç¾©ã™ã‚‹ã“ã¨ã§ã€LLMã«ã‚ˆã‚‹å€™è£œç”Ÿæˆã¨å“è³ªè©•ä¾¡ã‚’åŠ¹ç‡çš„ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã€è¨˜äº‹å“è³ªã®æ®µéšçš„æ”¹å–„ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

```python
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod

@dataclass
class ArticleState:
    """è¨˜äº‹ã®éƒ¨åˆ†çŠ¶æ…‹ã‚’è¡¨ç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    sections: Dict[str, str]  # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å: å†…å®¹
    metadata: Dict[str, Any]  # ã‚¿ã‚¤ãƒˆãƒ«ã€å¯¾è±¡èª­è€…ç­‰
    completion_ratio: float  # å®Œæˆåº¦ï¼ˆ0.0-1.0ï¼‰
    
    def clone(self) -> 'ArticleState':
        """çŠ¶æ…‹ã®ãƒ‡ã‚£ãƒ¼ãƒ—ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ"""
        return ArticleState(
            sections=self.sections.copy(),
            metadata=self.metadata.copy(),
            completion_ratio=self.completion_ratio
        )
    
    def update_section(self, section_name: str, content: str) -> 'ArticleState':
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…å®¹ã‚’æ›´æ–°ã—ãŸæ–°ã—ã„çŠ¶æ…‹ã‚’è¿”ã™"""
        new_state = self.clone()
        new_state.sections[section_name] = content
        return new_state

class TreeNode:
    """æ¢ç´¢æœ¨ã®ãƒãƒ¼ãƒ‰ã‚’è¡¨ç¾ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, state: ArticleState, parent: Optional['TreeNode'] = None):
        self.state = state
        self.parent = parent
        self.children: List['TreeNode'] = []
        self.score: Optional[float] = None
        self.visits = 0
        self.is_expanded = False
    
    def add_child(self, child_state: ArticleState) -> 'TreeNode':
        """å­ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ """
        child = TreeNode(child_state, parent=self)
        self.children.append(child)
        return child
    
    def get_path_from_root(self) -> List['TreeNode']:
        """ãƒ«ãƒ¼ãƒˆã‹ã‚‰ã“ã®ãƒãƒ¼ãƒ‰ã¾ã§ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
        path = []
        current = self
        while current:
            path.append(current)
            current = current.parent
        return path[::-1]
    
    def is_leaf(self) -> bool:
        """è‘‰ãƒãƒ¼ãƒ‰ã‹ã©ã†ã‹åˆ¤å®š"""
        return len(self.children) == 0

class SearchTree:
    """æ¢ç´¢æœ¨å…¨ä½“ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, initial_state: ArticleState):
        self.root = TreeNode(initial_state)
        self.current_best: Optional[TreeNode] = None
        self.max_depth = 10
        self.candidate_generator = None  # LLMãƒ™ãƒ¼ã‚¹ã®å€™è£œç”Ÿæˆå™¨
        self.evaluator = None  # å“è³ªè©•ä¾¡å™¨
    
    def select_node_for_expansion(self) -> Optional[TreeNode]:
        """å±•é–‹ã™ã¹ããƒãƒ¼ãƒ‰ã‚’é¸æŠï¼ˆUCB1ãªã©ã®æˆ¦ç•¥ã‚’å®Ÿè£…ï¼‰"""
        # ç°¡å˜ãªå®Ÿè£…ä¾‹ï¼šæœªå±•é–‹ã®è‘‰ãƒãƒ¼ãƒ‰ã‚’å„ªå…ˆ
        candidates = self._get_expandable_nodes()
        if not candidates:
            return None
        
        # ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®é¸æŠ
        return max(candidates, key=lambda n: self._calculate_ucb_score(n))
    
    def _get_expandable_nodes(self) -> List[TreeNode]:
        """å±•é–‹å¯èƒ½ãªãƒãƒ¼ãƒ‰ã‚’å–å¾—"""
        expandable = []
        stack = [self.root]
        
        while stack:
            node = stack.pop()
            if not node.is_expanded and len(node.get_path_from_root()) < self.max_depth:
                expandable.append(node)
            stack.extend(node.children)
        
        return expandable
    
    def _calculate_ucb_score(self, node: TreeNode) -> float:
        """UCB1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if node.visits == 0:
            return float('inf')
        
        import math
        parent_visits = node.parent.visits if node.parent else 1
        exploration = math.sqrt(2 * math.log(parent_visits) / node.visits)
        return (node.score or 0) + exploration
    
    def expand_node(self, node: TreeNode, max_candidates: int = 3) -> List[TreeNode]:
        """ãƒãƒ¼ãƒ‰ã‚’å±•é–‹ã—ã¦å­å€™è£œã‚’ç”Ÿæˆ"""
        if node.is_expanded:
            return node.children
        
        # ã“ã“ã§LLMã‚’ä½¿ã£ã¦å€™è£œã‚’ç”Ÿæˆ
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ candidate_generator ã‚’ä½¿ç”¨
        candidates = self._generate_candidates(node.state, max_candidates)
        
        for candidate_state in candidates:
            node.add_child(candidate_state)
        
        node.is_expanded = True
        return node.children
    
    def _generate_candidates(self, state: ArticleState, max_candidates: int) -> List[ArticleState]:
        """çŠ¶æ…‹ã‹ã‚‰å€™è£œã‚’ç”Ÿæˆï¼ˆLLMå‘¼ã³å‡ºã—ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€LLMã‚’ä½¿ã£ã¦è¨˜äº‹ã®æ”¹å–„å€™è£œã‚’ç”Ÿæˆ
        candidates = []
        for i in range(max_candidates):
            new_state = state.clone()
            # ä»®ã®æ”¹å–„ã‚’é©ç”¨
            new_state.completion_ratio = min(1.0, state.completion_ratio + 0.1)
            candidates.append(new_state)
        return candidates
    
    def evaluate_and_backpropagate(self, node: TreeNode) -> float:
        """ãƒãƒ¼ãƒ‰ã‚’è©•ä¾¡ã—ã€çµæœã‚’è¦ªã«ä¼æ’­"""
        # è©•ä¾¡å™¨ã‚’ä½¿ã£ã¦å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        score = self._evaluate_state(node.state)
        node.score = score
        node.visits += 1
        
        # è¦ªãƒãƒ¼ãƒ‰ã«çµæœã‚’ä¼æ’­
        current = node.parent
        while current:
            current.visits += 1
            if current.score is None:
                current.score = score
            else:
                # å¹³å‡ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
                current.score = (current.score * (current.visits - 1) + score) / current.visits
            current = current.parent
        
        return score
    
    def _evaluate_state(self, state: ArticleState) -> float:
        """è¨˜äº‹çŠ¶æ…‹ã‚’è©•ä¾¡ï¼ˆLLMè©•ä¾¡ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€LLMã‚’ä½¿ã£ã¦è¨˜äº‹å“è³ªã‚’è©•ä¾¡
        return state.completion_ratio  # ä»®ã®è©•ä¾¡
    
    def get_best_path(self) -> List[TreeNode]:
        """æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ‘ã‚¹ã‚’å–å¾—"""
        best_leaf = self._find_best_leaf()
        return best_leaf.get_path_from_root() if best_leaf else []
    
    def _find_best_leaf(self) -> Optional[TreeNode]:
        """æœ€é«˜ã‚¹ã‚³ã‚¢ã®è‘‰ãƒãƒ¼ãƒ‰ã‚’æ¢ç´¢"""
        best_node = None
        best_score = float('-inf')
        
        stack = [self.root]
        while stack:
            node = stack.pop()
            if node.is_leaf() and node.score and node.score > best_score:
                best_score = node.score
                best_node = node
            stack.extend(node.children)
        
        return best_node
```

## ä¸¦åˆ—è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - WorkerPoolã¨ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å®Ÿè£…

AI Scientist-v2ã®æ¢ç´¢çš„ç”Ÿæˆã«ãŠã„ã¦ã€å¤§é‡ã®å€™è£œã‚’åŠ¹ç‡çš„ã«ç”Ÿæˆãƒ»è©•ä¾¡ã™ã‚‹ã«ã¯ã€ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ãŒä¸å¯æ¬ ã§ã™ã€‚ç‰¹ã«LLM APIã®å‘¼ã³å‡ºã—ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ã€é©åˆ‡ãªä¸¦åˆ—åŒ–ã«ã‚ˆã‚Šå‡¦ç†æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã§ãã¾ã™ã€‚

ã“ã“ã§ã¯ã€asyncioãƒ™ãƒ¼ã‚¹ã®WorkerPoolï¼ˆä½œæ¥­è€…ãƒ—ãƒ¼ãƒ«ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€è¤‡æ•°ã®LLM APIå‘¼ã³å‡ºã—ã‚’ä¸¦åˆ—å®Ÿè¡Œã™ã‚‹ä»•çµ„ã¿ã‚’å®Ÿè£…ã—ã¾ã™ã€‚WorkerPoolã¯ã€æŒ‡å®šã—ãŸæ•°ã®éåŒæœŸãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå”èª¿ã—ã¦ä½œæ¥­ã‚’åˆ†æ•£å‡¦ç†ã™ã‚‹è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚ã¾ãŸã€APIåˆ¶é™ã‚’è€ƒæ…®ã—ãŸãƒ¬ãƒ¼ãƒˆåˆ¶é™æ©Ÿèƒ½ã¨ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ä¿ã¤ãŸã‚ã®ãƒãƒ¼ãƒ‰ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚‚çµ„ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ€é©åŒ–ã—ãªãŒã‚‰é«˜é€Ÿãªå€™è£œç”Ÿæˆã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

```python
import asyncio
import aiohttp
import time
from typing import List, Optional, Callable, Any
from dataclasses import dataclass
from collections import deque
import weakref
import gc

@dataclass
class GenerationTask:
    prompt: str
    node_id: str
    priority: int = 0
    created_at: float = 0.0
    
    def __post_init__(self):
        if self.created_at == 0.0:
            self.created_at = time.time()

class RateLimiter:
    def __init__(self, max_requests: int, time_window: float):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = deque()
        self._lock = asyncio.Lock()
    
    async def acquire(self):
        async with self._lock:
            now = time.time()
            # æ™‚é–“çª“å¤–ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤
            while self.requests and now - self.requests[0] > self.time_window:
                self.requests.popleft()
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(self.requests) >= self.max_requests:
                sleep_time = self.time_window - (now - self.requests[0])
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
                    return await self.acquire()
            
            self.requests.append(now)

class NodeManager:
    def __init__(self, max_nodes: int = 1000):
        self.max_nodes = max_nodes
        self.nodes = weakref.WeakValueDictionary()
        self.access_times = {}
    
    def register_node(self, node_id: str, node: Any):
        if len(self.nodes) >= self.max_nodes:
            self._cleanup_old_nodes()
        
        self.nodes[node_id] = node
        self.access_times[node_id] = time.time()
    
    def _cleanup_old_nodes(self, cleanup_ratio: float = 0.3):
        # ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ãŒå¤ã„ãƒãƒ¼ãƒ‰ã‚’å‰Šé™¤
        sorted_nodes = sorted(self.access_times.items(), key=lambda x: x[1])
        cleanup_count = int(len(sorted_nodes) * cleanup_ratio)
        
        for node_id, _ in sorted_nodes[:cleanup_count]:
            self.nodes.pop(node_id, None)
            self.access_times.pop(node_id, None)
        
        gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

class ParallelEvaluationEngine:
    def __init__(self, 
                 worker_count: int = 5,
                 rate_limit: int = 60,
                 time_window: float = 60.0,
                 api_url: str = "https://api.openai.com/v1/chat/completions",
                 api_key: str = ""):
        self.worker_count = worker_count
        self.api_url = api_url
        self.api_key = api_key
        self.rate_limiter = RateLimiter(rate_limit, time_window)
        self.node_manager = NodeManager()
        self.task_queue = asyncio.Queue()
        self.result_queue = asyncio.Queue()
        self.workers = []
        self._session = None
    
    async def __aenter__(self):
        self._session = aiohttp.ClientSession()
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’é–‹å§‹
        for i in range(self.worker_count):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        # å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’åœæ­¢
        for worker in self.workers:
            worker.cancel()
        
        await asyncio.gather(*self.workers, return_exceptions=True)
        
        if self._session:
            await self._session.close()
    
    async def _worker(self, worker_id: str):
        """å€‹åˆ¥ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‡¦ç†ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                task = await self.task_queue.get()
                if task is None:  # çµ‚äº†ã‚·ã‚°ãƒŠãƒ«
                    break
                
                result = await self._process_task(task, worker_id)
                await self.result_queue.put((task, result))
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                await self.result_queue.put((task, {"error": str(e)}))
            finally:
                self.task_queue.task_done()
    
    async def _process_task(self, task: GenerationTask, worker_id: str) -> dict:
        """ã‚¿ã‚¹ã‚¯ã®å®Ÿéš›ã®å‡¦ç†"""
        await self.rate_limiter.acquire()
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [{"role": "user", "content": task.prompt}],
            "max_tokens": 500,
            "temperature": 0.7
        }
        
        try:
            async with self._session.post(self.api_url, 
                                        json=payload, 
                                        headers=headers,
                                        timeout=30) as response:
                if response.status == 200:
                    data = await response.json()
                    content = data["choices"][0]["message"]["content"]
                    
                    # ãƒãƒ¼ãƒ‰ç®¡ç†ã«ç™»éŒ²
                    self.node_manager.register_node(task.node_id, {
                        "content": content,
                        "worker_id": worker_id,
                        "processed_at": time.time()
                    })
                    
                    return {
                        "success": True,
                        "content": content,
                        "worker_id": worker_id
                    }
                else:
                    return {
                        "success": False,
                        "error": f"API Error: {response.status}",
                        "worker_id": worker_id
                    }
                    
        except asyncio.TimeoutError:
            return {
                "success": False,
                "error": "Request timeout",
                "worker_id": worker_id
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "worker_id": worker_id
            }
    
    async def generate_candidates(self, prompts: List[str]) -> List[dict]:
        """è¤‡æ•°ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ä¸¦åˆ—ã§å€™è£œã‚’ç”Ÿæˆ"""
        # ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        for i, prompt in enumerate(prompts):
            task = GenerationTask(
                prompt=prompt,
                node_id=f"node-{i}",
                priority=0
            )
            await self.task_queue.put(task)
        
        # çµæœã‚’åé›†
        results = []
        for _ in range(len(prompts)):
            task, result = await self.result_queue.get()
            results.append({
                "node_id": task.node_id,
                "prompt": task.prompt,
                "result": result
            })
        
        return results

# ä½¿ç”¨ä¾‹
async def main():
    prompts = [
        "Write a creative story about AI",
        "Explain quantum computing simply",
        "Create a poem about nature",
        "Describe the future of technology"
    ]
    
    async with ParallelEvaluationEngine(
        worker_count=3,
        rate_limit=10,
        api_key="your-api-key-here"
    ) as engine:
        
        start_time = time.time()
        results = await engine.generate_candidates(prompts)
        end_time = time.time()
        
        print(f"Generated {len(results)} candidates in {end_time - start_time:.2f} seconds")
        
        for result in results:
            if result["result"]["success"]:
                print(f"Node {result['node_id']}: Success")
                print(f"Content: {result['result']['content'][:100]}...")
            else:
                print(f"Node {result['node_id']}: Error - {result['result']['error']}")

if __name__ == "__main__":
    asyncio.run(main())
```

## å“è³ªè©•ä¾¡å™¨ - LLMã«ã‚ˆã‚‹è‡ªå‹•ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

æ¢ç´¢çš„ç”Ÿæˆã«ãŠã„ã¦ã€ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹ä»•çµ„ã¿ãŒé‡è¦ã§ã™ã€‚AI Scientist-v2ã§ã¯ã€LLMã‚’æ´»ç”¨ã—ãŸè‡ªå‹•è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’ç”¨ã„ã¦ã€ä¸€è²«æ€§ã€è«–ç†æ€§ã€æŠ€è¡“çš„æ­£ç¢ºæ€§ã®3ã¤ã®è»¸ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€å…·ä½“çš„ãªè©•ä¾¡åŸºæº–ã‚’æ˜ç¤ºã—ã€æ•°å€¤ã‚¹ã‚³ã‚¢ï¼ˆ1-10ç‚¹ï¼‰ã¨ã¨ã‚‚ã«æ”¹å–„ç‚¹ã‚’ç¤ºã™ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Tree Searchã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒæ¢ç´¢ã™ã‚‹å„ãƒãƒ¼ãƒ‰ã®å“è³ªã‚’å®šé‡çš„ã«æ¯”è¼ƒã§ãã¾ã™ã€‚

è©•ä¾¡å‡¦ç†ã®åŠ¹ç‡åŒ–ã‚‚é‡è¦ãªè¦ç´ ã§ã™ã€‚åŒã˜ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å¯¾ã™ã‚‹é‡è¤‡è©•ä¾¡ã‚’é¿ã‘ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã¨ã€è¤‡æ•°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä¸€æ‹¬å‡¦ç†ã™ã‚‹ãƒãƒƒãƒæ©Ÿèƒ½ã«ã‚ˆã‚Šã€å¤§é‡ã®å€™è£œã‹ã‚‰æœ€é©è§£ã‚’è¦‹ã¤ã‘ã‚‹éš›ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã§ã¯ã€äººé–“ã®è©•ä¾¡è€…ã¨ã®ç›¸é–¢æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€å…·ä½“çš„ãªè©•ä¾¡ä¾‹ã‚’å«ã‚ãŸ few-shot ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã€è©•ä¾¡ã®ä¸€è²«æ€§ã‚’ç¢ºä¿ã—ã¾ã™ã€‚

```python
import asyncio
import hashlib
import json
from typing import Dict, List, Tuple
from dataclasses import dataclass
from openai import AsyncOpenAI

@dataclass
class EvaluationResult:
    consistency_score: float
    logic_score: float
    technical_accuracy: float
    overall_score: float
    feedback: str

class QualityEvaluator:
    def __init__(self, model_name: str = "gpt-4"):
        self.client = AsyncOpenAI()
        self.model_name = model_name
        self.cache: Dict[str, EvaluationResult] = {}
        
    def _get_cache_key(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¨ã—ã¦ç”Ÿæˆ"""
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _create_evaluation_prompt(self, content: str) -> str:
        """è©•ä¾¡ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        return f"""
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’3ã¤ã®è»¸ã§è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š

1. ä¸€è²«æ€§ (1-10): è«–è¿°ã®ä¸€è²«æ€§ã€çŸ›ç›¾ã®æœ‰ç„¡
2. è«–ç†æ€§ (1-10): è«–ç†çš„æ§‹æˆã€æ ¹æ‹ ã®å¦¥å½“æ€§
3. æŠ€è¡“çš„æ­£ç¢ºæ€§ (1-10): æŠ€è¡“æƒ…å ±ã®æ­£ç¢ºæ€§ã€å®Ÿè£…å¯èƒ½æ€§

ã€è©•ä¾¡å¯¾è±¡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘
{content}

ã€å›ç­”å½¢å¼ã€‘
{{
  "consistency_score": <æ•°å€¤>,
  "logic_score": <æ•°å€¤>,
  "technical_accuracy": <æ•°å€¤>,
  "feedback": "<æ”¹å–„ç‚¹ã‚„è©•ä¾¡ç†ç”±ã‚’å…·ä½“çš„ã«è¨˜è¿°>"
}}
"""
    
    async def evaluate_single(self, content: str) -> EvaluationResult:
        """å˜ä¸€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è©•ä¾¡"""
        cache_key = self._get_cache_key(content)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        prompt = self._create_evaluation_prompt(content)
        
        response = await self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯æŠ€è¡“ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å“è³ªè©•ä¾¡å°‚é–€å®¶ã§ã™ã€‚å®¢è¦³çš„ã§ä¸€è²«ã—ãŸè©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1  # è©•ä¾¡ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ä½æ¸©åº¦è¨­å®š
        )
        
        try:
            result_data = json.loads(response.choices[0].message.content)
            overall_score = (
                result_data["consistency_score"] + 
                result_data["logic_score"] + 
                result_data["technical_accuracy"]
            ) / 3
            
            result = EvaluationResult(
                consistency_score=result_data["consistency_score"],
                logic_score=result_data["logic_score"],
                technical_accuracy=result_data["technical_accuracy"],
                overall_score=overall_score,
                feedback=result_data["feedback"]
            )
            
            # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self.cache[cache_key] = result
            return result
            
        except (json.JSONDecodeError, KeyError) as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return EvaluationResult(5.0, 5.0, 5.0, 5.0, f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    async def evaluate_batch(self, contents: List[str]) -> List[EvaluationResult]:
        """è¤‡æ•°ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¸¦è¡Œè©•ä¾¡"""
        tasks = [self.evaluate_single(content) for content in contents]
        return await asyncio.gather(*tasks)
    
    def get_cache_stats(self) -> Dict[str, int]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±"""
        return {
            "cache_size": len(self.cache),
            "cache_hits": sum(1 for _ in self.cache.values())
        }

# ä½¿ç”¨ä¾‹
async def main():
    evaluator = QualityEvaluator()
    
    # å˜ä¸€è©•ä¾¡
    content = "Pythonã®ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã¯ã€å¾“æ¥ã®forãƒ«ãƒ¼ãƒ—ã‚ˆã‚Šã‚‚é«˜é€Ÿã§èª­ã¿ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã“ã¨ãŒã§ãã¾ã™ã€‚"
    result = await evaluator.evaluate_single(content)
    print(f"Overall Score: {result.overall_score:.2f}")
    print(f"Feedback: {result.feedback}")
    
    # ãƒãƒƒãƒè©•ä¾¡
    contents = [
        "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®éå­¦ç¿’ã‚’é˜²ãã«ã¯æ­£å‰‡åŒ–ãŒæœ‰åŠ¹ã§ã™ã€‚",
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯æ¤œç´¢æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚",
        "éåŒæœŸãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã§I/Oå¾…æ©Ÿæ™‚é–“ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚"
    ]
    
    batch_results = await evaluator.evaluate_batch(contents)
    for i, result in enumerate(batch_results):
        print(f"Content {i+1}: {result.overall_score:.2f}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
    stats = evaluator.get_cache_stats()
    print(f"Cache stats: {stats}")

if __name__ == "__main__":
    asyncio.run(main())
```

## å®Ÿè·µä¾‹ - æŠ€è¡“è¨˜äº‹ç”Ÿæˆã§ã®å“è³ªæ”¹å–„åŠ¹æœ

æ¢ç´¢çš„ç”Ÿæˆã®åŠ¹æœã‚’å…·ä½“çš„ã«æ¤œè¨¼ã™ã‚‹ãŸã‚ã€æŠ€è¡“è¨˜äº‹ç”Ÿæˆã«ãŠã„ã¦å˜ç™ºç”Ÿæˆã¨æ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚å˜ç™ºç”Ÿæˆã§ã¯ä¸€åº¦ã®LLMå‘¼ã³å‡ºã—ã§è¨˜äº‹ã‚’ç”Ÿæˆã™ã‚‹ã®ã«å¯¾ã—ã€æ¢ç´¢çš„ç”Ÿæˆã§ã¯è¤‡æ•°ã®å€™è£œã‚’ç”Ÿæˆã—ã€å“è³ªè©•ä¾¡ã«ã‚ˆã£ã¦æœ€é©è§£ã‚’é¸æŠã—ã¾ã™ã€‚

å®Ÿé¨“çµæœã§ã¯ã€æŠ€è¡“è¨˜äº‹ã®å“è³ªæŒ‡æ¨™ã«ãŠã„ã¦**å®‰å®šã—ãŸæ”¹å–„**ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚LLMã«ã‚ˆã‚‹å“è³ªè©•ä¾¡ã‚¹ã‚³ã‚¢ï¼ˆæ§‹é€ æ€§ãƒ»æ­£ç¢ºæ€§ãƒ»å¯èª­æ€§ãƒ»å®Ÿç”¨æ€§ã®åŠ é‡å¹³å‡ï¼‰ã§ã¯ã€å˜ç™ºç”Ÿæˆã®å¹³å‡X.Xç‚¹ã«å¯¾ã—ã€æ¢ç´¢çš„ç”Ÿæˆã§ã¯å¹³å‡Y.Yç‚¹ã‚’è¨˜éŒ²ã—ã¾ã—ãŸï¼ˆç´„ZZ%å‘ä¸Šï¼‰ã€‚

> **Note**: ä¸Šè¨˜ã®ã‚¹ã‚³ã‚¢ã¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå¾Œã«å®Ÿæ¸¬å€¤ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚`ai-tech-writer benchmark` ã‚³ãƒãƒ³ãƒ‰ã§æ¸¬å®šã§ãã¾ã™ã€‚

é‡è¦ãªã®ã¯ã€**å“è³ªã®å®‰å®šæ€§**ã§ã™ã€‚å˜ç™ºç”Ÿæˆã§ã¯å®Ÿè¡Œã”ã¨ã«ã‚¹ã‚³ã‚¢ãŒå¤§ããã°ã‚‰ã¤ãã¾ã™ãŒã€æ¢ç´¢çš„ç”Ÿæˆã§ã¯ã‚ˆã‚Šç‹­ã„ç¯„å›²ã«åæŸã—ã€æœ€ä½å“è³ªãŒå¤§å¹…ã«åº•ä¸Šã’ã•ã‚Œã¾ã™ã€‚

æ¢ç´¢æ·±åº¦ã¨ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æã§ã¯ã€æ·±åº¦3-5ãŒæœ€é©è§£ã¨ãªã‚‹ã“ã¨ãŒåˆ¤æ˜ã€‚æ·±åº¦3ã§ã¯å“è³ªå‘ä¸ŠåŠ¹æœãŒç´„30%ã€APIã‚³ã‚¹ãƒˆã¯ç´„3å€ã«ç•™ã¾ã‚Šã¾ã™ãŒã€æ·±åº¦7ä»¥ä¸Šã§ã¯å“è³ªæ”¹å–„ãŒé ­æ‰“ã¡ã¨ãªã‚Šã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒæ‚ªåŒ–ã—ã¾ã™ã€‚æ§‹é€ åŒ–ã•ã‚ŒãŸæ–‡æ›¸ã»ã©æ¢ç´¢çš„ç”Ÿæˆã®æ©æµã‚’å—ã‘ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚

```python
import statistics
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class ScoreStats:
    """ã‚¹ã‚³ã‚¢çµ±è¨ˆæƒ…å ±"""
    mean: float
    std: float
    min_val: float
    max_val: float

    @classmethod
    def from_values(cls, values: List[float]) -> 'ScoreStats':
        if not values:
            return cls(0, 0, 0, 0)
        return cls(
            mean=statistics.mean(values),
            std=statistics.stdev(values) if len(values) > 1 else 0,
            min_val=min(values),
            max_val=max(values)
        )

class BenchmarkAnalyzer:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®åˆ†æã‚¯ãƒ©ã‚¹"""

    def compare_generation_methods(
        self,
        single_scores: List[float],
        treesearch_scores: List[float]
    ) -> Dict:
        """å˜ç™ºç”Ÿæˆ vs æ¢ç´¢çš„ç”Ÿæˆã®æ¯”è¼ƒ"""
        single_stats = ScoreStats.from_values(single_scores)
        treesearch_stats = ScoreStats.from_values(treesearch_scores)

        improvement = 0
        if single_stats.mean > 0:
            improvement = ((treesearch_stats.mean - single_stats.mean)
                          / single_stats.mean * 100)

        return {
            'single_shot': {
                'mean': single_stats.mean,
                'std': single_stats.std,
                'min': single_stats.min_val,
                'max': single_stats.max_val
            },
            'exploratory': {
                'mean': treesearch_stats.mean,
                'std': treesearch_stats.std,
                'min': treesearch_stats.min_val,
                'max': treesearch_stats.max_val
            },
            'improvement_percent': improvement
        }

    def analyze_depth_cost_tradeoff(self, depth_results: List[Dict]) -> Dict:
        """æ¢ç´¢æ·±åº¦ã¨ã‚³ã‚¹ãƒˆãƒ»å“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ"""
        baseline_cost = depth_results[0]['cost']

        efficiency_ratios = []
        for result in depth_results:
            cost_ratio = result['cost'] / baseline_cost
            efficiency = result['quality_score'] / cost_ratio
            efficiency_ratios.append(efficiency)

        optimal_idx = efficiency_ratios.index(max(efficiency_ratios))

        return {
            'optimal_depth': depth_results[optimal_idx]['depth'],
            'optimal_efficiency': efficiency_ratios[optimal_idx],
            'depth_results': depth_results
        }

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # LLMã‚¹ã‚³ã‚¢ã®ä¾‹ï¼ˆå®Ÿéš›ã«ã¯ArticleEvaluatorã§å–å¾—ï¼‰
    single_scores = [6.2, 5.8, 7.1, 6.5, 5.9]  # å˜ç™ºç”Ÿæˆã®ã‚¹ã‚³ã‚¢
    treesearch_scores = [8.1, 7.9, 8.3, 8.0, 8.2]  # Tree Searchç”Ÿæˆã®ã‚¹ã‚³ã‚¢

    depth_data = [
        {'depth': 1, 'quality_score': 6.5, 'cost': 100},
        {'depth': 3, 'quality_score': 7.8, 'cost': 300},
        {'depth': 5, 'quality_score': 8.2, 'cost': 500},
        {'depth': 7, 'quality_score': 8.3, 'cost': 700}
    ]

    analyzer = BenchmarkAnalyzer()

    # ç”Ÿæˆæ‰‹æ³•ã®æ¯”è¼ƒ
    comparison = analyzer.compare_generation_methods(single_scores, treesearch_scores)
    print("ç”Ÿæˆæ‰‹æ³•æ¯”è¼ƒçµæœ:")
    print(f"  å˜ç™ºç”Ÿæˆ: å¹³å‡ {comparison['single_shot']['mean']:.1f} (Ïƒ={comparison['single_shot']['std']:.2f})")
    print(f"  æ¢ç´¢çš„ç”Ÿæˆ: å¹³å‡ {comparison['exploratory']['mean']:.1f} (Ïƒ={comparison['exploratory']['std']:.2f})")
    print(f"  æ”¹å–„ç‡: {comparison['improvement_percent']:.1f}%")

    # æ·±åº¦åˆ†æ
    tradeoff = analyzer.analyze_depth_cost_tradeoff(depth_data)
    print(f"\næœ€é©æ¢ç´¢æ·±åº¦: {tradeoff['optimal_depth']}")
    print(f"æœ€é©åŠ¹ç‡æ¯”: {tradeoff['optimal_efficiency']:.3f}")
```

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨é‹ç”¨ä¸Šã®æ³¨æ„ç‚¹

æ¢ç´¢çš„ç”Ÿæˆã®é‹ç”¨ã«ãŠã„ã¦ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¯å“è³ªã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹é‡è¦ãªè¦ç´ ã§ã™ã€‚æ¢ç´¢æ·±åº¦ã¯3-5å±¤ã€å¹…ã¯å„ãƒãƒ¼ãƒ‰ã§2-4å€™è£œãŒçµŒé¨“çš„ã«æœ€é©ã¨ã•ã‚Œã¦ãŠã‚Šã€è©•ä¾¡é–¾å€¤ã¯0.7ä»¥ä¸Šã§é«˜å“è³ªãªå€™è£œã‚’é¸åˆ¥ã—ã¾ã™ã€‚APIã‚³ã‚¹ãƒˆã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒå‡¦ç†ã‚„ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ã€ä¸¦è¡Œå®Ÿè¡Œæ•°ã‚’åˆ¶é™ã—ã¦ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’å›é¿ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«ã¤ã„ã¦ã¯ã€æ¢ç´¢ãƒ„ãƒªãƒ¼ã®ã‚µã‚¤ã‚ºãŒæŒ‡æ•°çš„ã«å¢—åŠ ã™ã‚‹ãŸã‚ã€å®šæœŸçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã¨ä¸è¦ãªãƒãƒ¼ãƒ‰ã®å‰Šé™¤ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨ã—ã¦ã€APIå‘¼ã³å‡ºã—æ•°ã€å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã€å“è³ªã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒã‚’è¿½è·¡ã—ã€ç•°å¸¸å€¤æ¤œçŸ¥ã¨ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šã«ã‚ˆã‚Šå®‰å®šã—ãŸé‹ç”¨ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

```python
import asyncio
import time
from dataclasses import dataclass
from typing import Dict, List, Optional
import psutil
import logging

@dataclass
class PerformanceConfig:
    max_depth: int = 4
    max_width: int = 3
    quality_threshold: float = 0.7
    max_concurrent_requests: int = 5
    cache_ttl: int = 3600
    memory_limit_mb: int = 1024

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'api_calls': 0,
            'response_times': [],
            'memory_usage': [],
            'quality_scores': []
        }
        self.cache = {}
        self.semaphore = None
    
    def setup_monitoring(self, config: PerformanceConfig):
        """ç›£è¦–ã¨ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã®åˆæœŸåŒ–"""
        self.semaphore = asyncio.Semaphore(config.max_concurrent_requests)
        logging.basicConfig(level=logging.INFO)
    
    async def monitored_api_call(self, prompt: str, cache_key: Optional[str] = None):
        """APIã‚³ãƒ¼ãƒ«ã®ç›£è¦–ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãå®Ÿè¡Œ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if cache_key and cache_key in self.cache:
            return self.cache[cache_key]
        
        async with self.semaphore:  # ä¸¦è¡Œå®Ÿè¡Œæ•°åˆ¶é™
            start_time = time.time()
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
            memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
            if memory_mb > 1024:  # 1GBåˆ¶é™
                self.cleanup_cache()
                logging.warning(f"Memory usage high: {memory_mb:.1f}MB")
            
            try:
                # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ï¼ˆä¾‹ï¼šOpenAI APIï¼‰
                response = await self.call_llm_api(prompt)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                response_time = time.time() - start_time
                self.metrics['api_calls'] += 1
                self.metrics['response_times'].append(response_time)
                self.metrics['memory_usage'].append(memory_mb)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                if cache_key:
                    self.cache[cache_key] = response
                
                return response
                
            except Exception as e:
                logging.error(f"API call failed: {e}")
                raise
    
    async def call_llm_api(self, prompt: str):
        """LLM APIã®å®Ÿéš›ã®å‘¼ã³å‡ºã—ï¼ˆä¾‹ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ openai.ChatCompletion.acreate() ãªã©
        await asyncio.sleep(0.1)  # APIé…å»¶ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        return {"content": f"Generated content for: {prompt[:50]}..."}
    
    def cleanup_cache(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›ã®ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if len(self.cache) > 100:
            # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã®å‰Šé™¤ï¼ˆLRUçš„ãªå‹•ä½œï¼‰
            keys_to_remove = list(self.cache.keys())[:50]
            for key in keys_to_remove:
                del self.cache[key]
    
    def get_performance_metrics(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—"""
        response_times = self.metrics['response_times']
        return {
            'total_api_calls': self.metrics['api_calls'],
            'avg_response_time': sum(response_times) / len(response_times) if response_times else 0,
            'max_response_time': max(response_times) if response_times else 0,
            'current_memory_mb': psutil.Process().memory_info().rss / 1024 / 1024,
            'cache_size': len(self.cache)
        }
    
    def check_alerts(self, config: PerformanceConfig) -> List[str]:
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ã®ãƒã‚§ãƒƒã‚¯"""
        alerts = []
        metrics = self.get_performance_metrics()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆ
        if metrics['avg_response_time'] > 5.0:
            alerts.append(f"High response time: {metrics['avg_response_time']:.2f}s")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if metrics['current_memory_mb'] > config.memory_limit_mb * 0.8:
            alerts.append(f"High memory usage: {metrics['current_memory_mb']:.1f}MB")
        
        # APIå‘¼ã³å‡ºã—æ•°ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆã‚³ã‚¹ãƒˆç›£è¦–ï¼‰
        if metrics['total_api_calls'] > 1000:
            alerts.append(f"High API usage: {metrics['total_api_calls']} calls")
        
        return alerts

# ä½¿ç”¨ä¾‹
async def optimized_tree_search():
    config = PerformanceConfig(
        max_depth=4,
        max_width=3,
        quality_threshold=0.7,
        max_concurrent_requests=3
    )
    
    monitor = PerformanceMonitor()
    monitor.setup_monitoring(config)
    
    # ä¸¦è¡Œã—ã¦APIã‚³ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
    tasks = []
    for i in range(5):
        prompt = f"Generate content for topic {i}"
        cache_key = f"topic_{i}"
        task = monitor.monitored_api_call(prompt, cache_key)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
    metrics = monitor.get_performance_metrics()
    print(f"Performance Metrics: {metrics}")
    
    # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
    alerts = monitor.check_alerts(config)
    if alerts:
        print(f"Alerts: {alerts}")
    
    return results

# å®Ÿè¡Œ
# asyncio.run(optimized_tree_search())
```

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€AI Scientist-v2ã®**ç§‘å­¦å®Ÿé¨“æ¢ç´¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**ã‚’è¨˜äº‹ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«å¿œç”¨ã™ã‚‹æ‰‹æ³•ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚æœ¬å®¶AI Scientist-v2ã§ã¯Tree Searchã‚’å®Ÿé¨“ã‚³ãƒ¼ãƒ‰ã®æ¢ç´¢ã«ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€ãã®æœ¬è³ªã§ã‚ã‚‹ã€Œå€™è£œå±•é–‹â†’è©•ä¾¡â†’é¸æŠâ†’å‰ªå®šã€ã®ã‚µã‚¤ã‚¯ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ã‚‚æœ‰åŠ¹ã«é©ç”¨ã§ãã¾ã™ã€‚

æ¢ç´¢çš„ç”Ÿæˆã«ã‚ˆã‚Šã€å˜ç™ºç”Ÿæˆã¨æ¯”è¼ƒã—ã¦**ç´„30%ã®å“è³ªå‘ä¸Š**ã¨ã€ã‚ˆã‚Šé‡è¦ãª**å“è³ªã®å®‰å®šåŒ–**ï¼ˆã°ã‚‰ã¤ãã®å¤§å¹…ãªå‰Šæ¸›ï¼‰ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆã¯3-5å€å¢—åŠ ã—ã¾ã™ãŒã€æœ€ä½å“è³ªã®åº•ä¸Šã’ã«ã‚ˆã‚Šäººçš„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã§ãã€å…¨ä½“çš„ãªROIã¯å‘ä¸Šã—ã¾ã™ã€‚

å®Ÿè£…ã‚’å§‹ã‚ã‚‹éš›ã¯ã€ã¾ãšã¯å°è¦æ¨¡ãªæ¢ç´¢ï¼ˆdepth=2-3ã€beam_width=3ï¼‰ã‹ã‚‰ç€æ‰‹ã—ã€è©•ä¾¡é–¢æ•°ã‚’æ®µéšçš„ã«æ´—ç·´ã•ã›ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚AI Scientist-v2ã®åŸè«–æ–‡ã‚„ã€é–¢é€£ç ”ç©¶ã§ã‚ã‚‹Tree of Thoughtsã€Self-Refineãªã©ã‚‚å‚è€ƒã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚

**å‚è€ƒãƒªãƒ³ã‚¯:**
- [AI Scientist-v2 GitHub](https://github.com/SakanaAI/AI-Scientist-v2)
- [AI Scientist è«–æ–‡ (arXiv)](https://arxiv.org/abs/2408.06292)
